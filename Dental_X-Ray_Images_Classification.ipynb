{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10243814,"sourceType":"datasetVersion","datasetId":6335126}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies (run once)\n# If running on Kaggle, these may already be present.\n\n# Reproducibility & device\nimport random, os, torch, numpy as np\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device:', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T06:33:40.633108Z","iopub.execute_input":"2025-08-12T06:33:40.633601Z","iopub.status.idle":"2025-08-12T06:33:46.690924Z","shell.execute_reply.started":"2025-08-12T06:33:40.633576Z","shell.execute_reply":"2025-08-12T06:33:46.690261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Project : Dental X-Ray Images Classification\n\nThis notebook aims to detect and classify dental diseases from panoramic X-ray images using a Swin Transformer model with self-supervised learning (Swin-SSL).  \n\n**Key steps in this notebook:**\n- **Exploratory Data Analysis (EDA):** Inspect dataset distribution, visualize samples, and analyze grayscale intensity patterns.\n- **Data Preprocessing:** Apply image transformations suitable for model training.\n- **Model Implementation:** Integrate and fine-tune the Swin-SSL architecture for dental X-ray classification.\n- **Training & Validation:** Train the model, monitor performance, and adjust hyperparameters.\n- **Evaluation:** Assess the trained model’s accuracy, loss curves, and classification performance metrics.\n","metadata":{}},{"cell_type":"markdown","source":"First we'll install the required libraries first","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.transforms import RandAugment\n\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom timm import create_model\nfrom torchvision.datasets import ImageFolder\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import random_split\nfrom collections import Counter\nfrom timm.data import Mixup\nfrom torchvision import datasets, transforms\nimport pandas as pd\n\n\nfrom timm.loss import SoftTargetCrossEntropy  \nfrom torch.cuda.amp import autocast\n\nimport os\nimport shutil\nimport random\nimport cv2\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom albumentations import (\n    HorizontalFlip, RandomBrightnessContrast, Rotate, GaussianBlur, Compose\n)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-12T06:33:46.692098Z","iopub.execute_input":"2025-08-12T06:33:46.692607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Local FocalLoss implementation (replaces dependency on torchtoolbox)\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        # inputs: logits, targets: class indices\n        ce = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce)\n        loss = self.alpha * (1 - pt) ** self.gamma * ce\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Loading the dataset","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/dental-opg-xray-dataset/Dental OPG XRAY Dataset/Dental OPG (Classification)\"\n\nclasses = os.listdir(data_dir)\n\nprint(f\"Classes: {classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Displaying sample images","metadata":{}},{"cell_type":"code","source":"def plot_sample_images(class_name, n = 5):\n    folder = os.path.join(data_dir, class_name)\n    images = os.listdir(folder)[:n]\n    plt.figure(figsize=(15, 5))\n    for i, img_name in enumerate(images):\n        img_path = os.path.join(folder, img_name)\n        img = Image.open(img_path)\n        plt.subplot(1, n, i + 1)\n        plt.imshow(img.convert('L'), cmap='gray')\n        plt.title(class_name)\n        plt.axis('off')\n    plt.show()\n\n# Plot sample from each class\nfor cls in classes:\n    plot_sample_images(cls)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that majority of the images are of Health Teeth, while the least are of Fractured Teeth. There is a huge imbalance of classes in the dataset. This might create bias predictions. We will handle this later using data augmentation or class sampling.","metadata":{}},{"cell_type":"markdown","source":"### Image Size Analysis","metadata":{}},{"cell_type":"code","source":"images_sizes = []\nfor class_name in os.listdir(data_dir):\n    class_path = os.path.join(data_dir, class_name)\n    images_files = os.listdir(class_path)[:10]\n    for image_file in images_files:\n        img_path = os.path.join(class_path, image_file)\n        try:\n            with Image.open(img_path) as img:\n                images_sizes.append(img.size)\n\n        except:\n            continue\n\n# Split sizes\nwidths, heights = zip(*images_sizes)\n\n# Plot distributions\nplt.figure(figsize=(10, 5))\nplt.hist(widths, bins=10, alpha=0.7, label=\"Widths\", color='skyblue')\nplt.hist(heights, bins=10, alpha=0.7, label=\"Heights\", color='lightgreen')\nplt.title(\"Image Width and Height Distribution\")\nplt.xlabel(\"Pixels\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Image Size Analysis\n\nThe X-ray images show notable variation in dimensions, with widths ranging from 1000–1800 pixels and heights between 400–800 pixels. This confirms their panoramic nature.\n\nTo ensure consistent input shape for model training, all images will be resized to 224×224 pixels using standard transformations. Center cropping may also be applied to better preserve structure.\n\nStandardizing image size is essential for reliable and efficient model training.\n","metadata":{}},{"cell_type":"markdown","source":"### Gray Scale Pixel Analysis","metadata":{}},{"cell_type":"code","source":"sample_class = os.listdir(data_dir)[0]\nsample_img_path = os.path.join(data_dir, sample_class, os.listdir(os.path.join(data_dir, sample_class))[0])\n\nimage = Image.open(sample_img_path).convert('L')\nimage_np = np.array(image)\n\n# Show stats\nprint(f\"Min Pixel Value: {image_np.min()}\")\nprint(f\"Max Pixel Value: {image_np.max()}\")\nprint(f\"Mean Pixel Value: {image_np.mean():.2f}\")\n\n# Plot image and histogram\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(image_np, cmap='gray')\nplt.title(\"Sample Grayscale X-ray\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.hist(image_np.flatten(), bins=30, color='gray')\nplt.title(\"Pixel Intensity Distribution\")\nplt.xlabel(\"Pixel Value (0-255)\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The X-ray images are confirmed to be grayscale with pixel values ranging from **0 to 255**, and a mean intensity around **138**.\n\nThe distribution is fairly balanced, indicating good contrast across the dataset.\n\nThese insights support using standard grayscale normalization (`[0.5], [0.5]`) during preprocessing to stabilize training.","metadata":{}},{"cell_type":"markdown","source":"#### Dataset","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/dental-opg-xray-dataset/Dental OPG XRAY Dataset/Dental OPG (Classification)\"\nprint(os.listdir(dataset_path))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\nclass_counts = defaultdict(int)\n\n# Loop through class folders\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    if os.path.isdir(class_dir):\n        num_images = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        class_counts[class_name] = num_images\n\n# Print the results\nprint(f\"Total classes: {len(class_counts)}\\n\")\nfor cls, count in class_counts.items():\n    print(f\"{cls}: {count} images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import all Library","metadata":{}},{"cell_type":"code","source":"target_path = \"/kaggle/working/augmented_dataset\"\ntarget_per_class = 200\nsplit_ratio = 0.8  # 80% train, 20% val\nrandom.seed(42)\n\n# ========== AUGMENTATION TRANSFORMS ==========\ntransform = Compose([\n    HorizontalFlip(p=0.5),\n    Rotate(limit=15, p=0.7),\n    RandomBrightnessContrast(p=0.5),\n    GaussianBlur(blur_limit=3, p=0.3),\n])\n\ndef augment_image(img):\n    augmented = transform(image=img)\n    return augmented['image']\n\n\n# ========== CREATE AUGMENTED DATA ==========\nos.makedirs(target_path, exist_ok=True)\n\nfor class_name in os.listdir(dataset_path):\n    src_dir = os.path.join(dataset_path, class_name)\n    tgt_dir = os.path.join(target_path, class_name)\n    os.makedirs(tgt_dir, exist_ok=True)\n\n    images = [f for f in os.listdir(src_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n    image_paths = [os.path.join(src_dir, f) for f in images]\n\n    # Step 1: Copy originals\n    for img_file in images:\n        shutil.copy(os.path.join(src_dir, img_file), os.path.join(tgt_dir, img_file))\n\n    # Step 2: Generate more if needed\n    needed = target_per_class - len(images)\n    if needed <= 0:\n        continue\n\n    print(f\"[{class_name}] Augmenting {needed} images...\")\n\n    for i in tqdm(range(needed)):\n        rand_img_path = random.choice(image_paths)\n        img = cv2.imread(rand_img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        aug_img = augment_image(img)\n        aug_img = cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR)\n\n        out_path = os.path.join(tgt_dir, f\"aug_{i}_{os.path.basename(rand_img_path)}\")\n        cv2.imwrite(out_path, aug_img)\n\nprint(\"All classes now have 200 images each.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random.seed(42)\n\naugmented_path = \"/kaggle/working/augmented_dataset\"\nhealthy_class = \"Healthy Teeth\"\ntarget_healthy_count = 200\n\nhealthy_dir = os.path.join(augmented_path, healthy_class)\nall_healthy_images = os.listdir(healthy_dir)\n\nprint(f\"Original Healthy Teeth images: {len(all_healthy_images)}\")\n\nif len(all_healthy_images) > target_healthy_count:\n    # Randomly select images to keep\n    selected_images = random.sample(all_healthy_images, target_healthy_count)\n\n    # Remove unselected images\n    for img in all_healthy_images:\n        if img not in selected_images:\n            os.remove(os.path.join(healthy_dir, img))\n    print(f\"Downsampled Healthy Teeth images to: {target_healthy_count}\")\nelse:\n    print(\"No downsampling needed, already at or below target count.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset Augmentation, Preprocessing","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/working/augmented_dataset\"  \n\ndataset = ImageFolder(root=dataset_path)\n\nprint(\"Classes:\", dataset.classes)\nprint(\"Class-to-Index Mapping:\", dataset.class_to_idx)\n\n# Count number of samples per class index\nlabels = [sample[1] for sample in dataset.samples]\nclass_counts = Counter(labels)\n\n# Print class distribution nicely\nprint(\"Class Distribution:\")\nfor class_idx, count in class_counts.items():\n    print(f\"  {dataset.classes[class_idx]}: {count} images\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CLAHETransform using this dataset","metadata":{}},{"cell_type":"code","source":"class CLAHETransform:\n    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n    \n    def __call__(self, img):\n        img_np = np.array(img.convert(\"L\"))\n        img_clahe = self.clahe.apply(img_np)\n        return Image.fromarray(img_clahe).convert(\"RGB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\n\n# Define transformations\ntransform_train = transforms.Compose([\\n    transforms.Grayscale(num_output_channels=3), \n    CLAHETransform(),\n    transforms.RandomResizedCrop((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n])\n\ntransform_test = transforms.Compose([\\n    transforms.Grayscale(num_output_channels=3), \n    CLAHETransform(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n])\n\n# Load entire balanced dataset (200x6 = 1200 images)\ndataset = datasets.ImageFolder(root=\"/kaggle/working/augmented_dataset\", transform=transform_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load the dataset","metadata":{}},{"cell_type":"code","source":"# Split sizes (70% train, 20% val, 10% test)\ntotal_size = len(dataset)\ntrain_size = int(0.7 * total_size)\nval_size = int(0.2 * total_size)\ntest_size = total_size - train_size - val_size\n\n# Perform the split\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Apply transform_test to val and test sets\nval_dataset.dataset.transform = transform_test\ntest_dataset.dataset.transform = transform_test\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Confirm dataset sizes\nprint(\"Train Dataset Size:\", len(train_dataset))\nprint(\"Validation Dataset Size:\", len(val_dataset))\nprint(\"Test Dataset Size:\", len(test_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model: Swin Transformer (swin_tiny_patch4_window7_224)\n\nThis model is based on the **Swin Transformer** architecture with the following configuration:\n- **Patch Size:** 4 × 4  \n- **Window Size:** 7  \n- **Input Resolution:** 224 × 224 pixels  \n\nThe Swin Transformer is a hierarchical Vision Transformer that computes self-attention within shifted windows, enabling:\n- Efficient computation for high-resolution images\n- Strong performance across various vision tasks\n- Better locality and inductive bias compared to vanilla Vision Transformers\n\nIn this notebook, the `swin_tiny_patch4_window7_224` variant is used due to its lightweight design, making it suitable for medical imaging tasks while maintaining strong accuracy.\n","metadata":{}},{"cell_type":"code","source":"# Define model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=6,drop_path_rate=0.3)\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from timm.data import Mixup\nfrom timm.loss import SoftTargetCrossEntropy  # Required for mixup\nfrom torch.cuda.amp import autocast\n# Define optimizer, criterion, scaler\n\nmixup_fn = Mixup(\n    mixup_alpha=0.4,\n    cutmix_alpha=1.0,\n    cutmix_minmax=None,\n    prob=1.0,\n    switch_prob=0.5,\n    mode='batch',\n    label_smoothing=0.1,\n    num_classes=6\n)\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# class_weights = compute_class_weight(class_weight='balanced', classes=np.arange(6), y=val_labels)\n# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n# Get integer labels from the validation subset\nval_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else range(len(val_dataset))\nval_labels_full = [dataset.samples[i][1] for i in val_indices]\n\n# Compute class weights from val label distribution\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.arange(6), y=val_labels_full)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n#criterion = nn.CrossEntropyLoss()\n#criterion = FocalLoss(gamma=2.0,classes=6)\n#criterion = SoftTargetCrossEntropy()\n#train_criterion = FocalLoss(gamma=2.0,classes=6)\ntrain_criterion = SoftTargetCrossEntropy()  # for mixup\nval_criterion = nn.CrossEntropyLoss(weight=class_weights)       # for integer labels\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4,weight_decay=1e-5)\n\nfrom timm.scheduler import CosineLRScheduler\n\nscheduler = CosineLRScheduler(\n    optimizer,\n    t_initial=25,         # Set this to total number of epochs\n    lr_min=1e-6,          # Final learning rate\n    warmup_lr_init=1e-6,  # Starting LR for warmup\n    warmup_t=3,           # Number of warmup epochs\n    cycle_limit=1\n)\n\nscaler = torch.cuda.amp.GradScaler()\n\n\n\n# Training and validation functions\ndef train_one_epoch(model, loader, optimizer, criterion, scaler):\n    model.train()\n    total_loss, preds, labels = 0, [], []\n    for images, targets in loader:\n        images, targets = images.to(device), targets.to(device)\n        images, targets = mixup_fn(images, targets)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item() * images.size(0)\n        preds += torch.argmax(outputs, dim=1).cpu().tolist()\n        #labels += targets.cpu().tolist()\n        labels += torch.argmax(targets, dim=1).cpu().tolist()\n    avg_loss = total_loss / len(loader.dataset)\n    return avg_loss, preds, labels\n\ndef validate(model, loader, criterion):\n    model.eval()\n    total_loss, preds, labels = 0, [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n\n            total_loss += loss.item() * images.size(0)\n            preds += torch.argmax(outputs, dim=1).cpu().tolist()\n            labels += targets.cpu().tolist()\n    avg_loss = total_loss / len(loader.dataset)\n    return avg_loss, preds, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 25\ntrain_log = []\nval_log = []\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_preds, train_labels = train_one_epoch(model, train_loader, optimizer, train_criterion, scaler)\n    val_loss, val_preds, val_labels = validate(model, val_loader, val_criterion)\n\n    scheduler.step(epoch) \n    \n    train_acc = (torch.tensor(train_preds) == torch.tensor(train_labels)).float().mean().item()\n    val_acc = (torch.tensor(val_preds) == torch.tensor(val_labels)).float().mean().item()\n\n    train_f1 = f1_score(train_labels, train_preds, average='macro')\n    val_f1 = f1_score(val_labels, val_preds, average='macro')\n\n    train_log.append({\"epoch\": epoch, \"loss\": train_loss, \"acc\": train_acc, \"f1\": train_f1})\n    val_log.append({\"epoch\": epoch, \"loss\": val_loss, \"acc\": val_acc, \"f1\": val_f1})\n\n    if epoch % 5 == 0 or epoch == 1:\n        print(f\"Epoch {epoch}\")\n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n    else:\n        print(f\"Epoch {epoch}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n# Classification report for validation\nprint(\"\\nValidation Classification Report:\")\nprint(classification_report(val_labels, val_preds, target_names=dataset.classes))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract metrics from logs\nepochs = [log['epoch'] for log in train_log]\n\ntrain_losses = [log['loss'] for log in train_log]\nval_losses = [log['loss'] for log in val_log]\n\ntrain_accs = [log['acc'] for log in train_log]\nval_accs = [log['acc'] for log in val_log]\n\ntrain_f1s = [log['f1'] for log in train_log]\nval_f1s = [log['f1'] for log in val_log]\n\nplt.figure(figsize=(18, 5))\n\n# Loss curve\nplt.subplot(1, 3, 1)\nplt.plot(epochs, train_losses, label='Train Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\n# Accuracy curve\nplt.subplot(1, 3, 2)\nplt.plot(epochs, train_accs, label='Train Accuracy')\nplt.plot(epochs, val_accs, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Curve')\nplt.legend()\n\n# F1 score curve\nplt.subplot(1, 3, 3)\nplt.plot(epochs, train_f1s, label='Train F1 Score')\nplt.plot(epochs, val_f1s, label='Validation F1 Score')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.title('F1 Score Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_criterion = nn.CrossEntropyLoss() \ntest_loss, test_preds, test_labels = validate(model, test_loader, test_criterion)\nprint(\"\\nTest Classification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=dataset.classes))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DINO","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nimport copy\n\nclass DINOLoss(nn.Module):\n    def __init__(self, out_dim=65536, teacher_temp=0.07, student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.center_momentum = center_momentum\n        self.register_buffer('center', torch.zeros(1, out_dim))\n\n    def forward(self, student_output, teacher_output):\n        student_out = student_output / self.student_temp\n        student_out = F.log_softmax(student_out, dim=-1)\n\n        teacher_out = F.softmax((teacher_output - self.center) / self.teacher_temp, dim=-1)\n        teacher_out = teacher_out.detach()\n\n        loss = -torch.sum(teacher_out * student_out, dim=-1).mean()\n        self.update_center(teacher_output)\n        return loss\n\n    @torch.no_grad()\n    def update_center(self, teacher_output):\n        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)\n        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Swin Transformer for DINO (output 65536 dim)\nstudent = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=65536)\nteacher = copy.deepcopy(student)\nfor p in teacher.parameters():\n    p.requires_grad = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nstudent = student.to(device)\nteacher = teacher.to(device)\ndino_loss = DINOLoss().to(device)\n\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     student = nn.DataParallel(student)\n#     teacher = nn.DataParallel(teacher)\n#     dino_loss = nn.DataParallel(dino_loss)\n\noptimizer = torch.optim.AdamW(student.parameters(), lr=1e-4, weight_decay=1e-4)\n\n# Use your real DataLoader with images only (no labels needed)\nfrom timm.scheduler import CosineLRScheduler\n\n# After defining your optimizer\nscheduler = CosineLRScheduler(\n    optimizer,\n    t_initial=100,  # Total DINO epochs\n    lr_min=1e-6,    # Final learning rate\n    warmup_lr_init=1e-6,\n    warmup_t=10,    # Warmup for first 10 epochs\n    cycle_limit=1,\n)\nfor epoch in range(100):\n    student.train()\n    running_loss = 0.0\n    num_batches = 0\n\n    for images, _ in train_loader:  # Use labels only as placeholders\n        images = images.to(device)\n\n        student_out = student(images)\n        with torch.no_grad():\n            teacher_out = teacher(images)\n\n        loss = dino_loss(student_out, teacher_out)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        num_batches += 1\n\n    scheduler.step(epoch)\n    # EMA update\n    \n    with torch.no_grad():\n        for ps, pt in zip(student.parameters(), teacher.parameters()):\n            pt.data = 0.996 * pt.data + (1 - 0.996) * ps.data\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/100, Avg DINO Loss: {running_loss / num_batches:.4f}\")\n\n# Save pretrained backbone\ntorch.save(student.state_dict(), \"dino_pretrained_swin.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load DINO-pretrained model with classifier removed\nbackbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=0)\nbackbone.load_state_dict(torch.load(\"dino_pretrained_swin.pth\"), strict=False)\n\n# DINOClassifier assumes Swin-Tiny outputs 768-dim features\nclass DINOClassifier(nn.Module):\n    def __init__(self, backbone, feature_dim=768, num_classes=6):\n        super().__init__()\n        self.backbone = backbone\n        self.classifier = nn.Linear(feature_dim, num_classes)\n\n    def forward(self, x):\n        features = self.backbone(x)  # shape: [B, 768]\n        return self.classifier(features)\n\nmodel = DINOClassifier(backbone, feature_dim=768, num_classes=6).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n# Training Loop\nfor epoch in range(20):\n    model.train()\n    train_loss = 0.0\n    all_preds, all_labels = [], []\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    # Safe metric calculations\n    train_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n    train_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n    train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, labels in train_loader:\n    student_out = student(images.to(device))\n    print(\"Output shape:\", student_out.shape)  \n    preds = student_out.argmax(dim=1)\n    print(\"Sample preds:\", preds[:5].cpu().numpy())\n    print(\"Sample labels:\", labels[:5].numpy())\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_names = dataset.classes\nreport = classification_report(all_labels, all_preds, target_names=target_names, digits=4)\nprint(f\"\\n[Epoch {epoch+1}] Classification Report:\\n{report}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention Rollout","metadata":{}},{"cell_type":"code","source":"# --- Load image and preprocess ---\nimg_path = \"/kaggle/working/augmented_dataset/BDC-BDR/101.jpg\"\noriginal_image = Image.open(img_path).convert(\"RGB\")\ntransform = transforms.Compose([\\n    transforms.Grayscale(num_output_channels=3), \n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\ninput_tensor = transform(original_image).unsqueeze(0).to(device)  # shape: (1, 3, 224, 224)\n\n# --- Hook to extract attention weights ---\nattention_maps = []\n\ndef get_attention_hook(module, input, output):\n    # Output shape: (B, Heads, Tokens, Tokens)\n    attention_maps.append(output)\n\n# --- Register hooks on all attention layers ---\nhooks = []\nfor name, module in model.named_modules():\n    if 'attn.attn_drop' in name:  # Works well for Swin\n        hooks.append(module.register_forward_hook(get_attention_hook))\n\n# --- Forward pass ---\nwith torch.no_grad():\n    _ = model(input_tensor)\n\n# --- Clean up hooks ---\nfor hook in hooks:\n    hook.remove()\n\n# --- Compute Attention Rollout ---\n# Sum all heads and average across layers\ndef compute_rollout(attn_list):\n    result = torch.eye(attn_list[0].size(-1)).to(device)\n    for attn in attn_list:\n        attn_heads_fused = attn.mean(dim=1)  # Average over heads: (B, Tokens, Tokens)\n        attn_heads_fused = attn_heads_fused[0]  # Remove batch dim\n        attn_heads_fused += torch.eye(attn_heads_fused.size(0)).to(device)  # add residual\n        attn_heads_fused /= attn_heads_fused.sum(dim=-1, keepdim=True)\n        result = torch.matmul(attn_heads_fused, result)\n    return result\n\nrollout = compute_rollout(attention_maps)  # Shape: (tokens, tokens)\n\n# --- Convert rollout to spatial map ---\n# For Swin Transformer, patch count varies — \n# Get attention to image patches \nnum_patches = int(np.sqrt(rollout.shape[0]))  \nattn_map = rollout[0].reshape(num_patches, num_patches).cpu().numpy()\n\n\n# Resize attention to match image size\n\nattn_resized = cv2.resize(attn_map, (224, 224))\n\n# --- Normalize and overlay ---\nattn_normalized = (attn_resized - attn_resized.min()) / (attn_resized.max() - attn_resized.min())\nheatmap = np.uint8(255 * attn_normalized)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nheatmap = np.float32(heatmap) / 255\n\nrgb_image = np.array(original_image.resize((224, 224))).astype(np.float32) / 255.0\noverlay = heatmap * 0.5 + rgb_image * 0.5\n\n# --- Show Result ---\nplt.imshow(overlay)\nplt.title(\"Attention Rollout\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_true = all_labels\ny_pred = all_preds\n\n# Now plot confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install grad-cam --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\n# Choose the last convolutional layer for Swin Transformer\n# For swin_tiny_patch4_window7_224, you typically use the last block's norm layer\ntarget_layers = [model.layers[-1].blocks[-1].norm1]\n\ncam = GradCAM(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inverse normalization (if used during preprocessing)\ninv_normalize = transforms.Normalize(\n    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n    std=[1/0.229, 1/0.224, 1/0.225]\n)\n\n# Pick a few samples from validation set\nmodel.eval()\nsamples_to_show = 5\n\nfor i in range(samples_to_show):\n    image, label = val_dataset[i]\n    input_tensor = image.unsqueeze(0).to(device)\n    \n    # Get prediction\n    output = model(input_tensor)\n    pred_label = torch.argmax(output, 1).item()\n    \n    # Generate Grad-CAM\n    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_label)])[0, :]\n    \n    # Convert tensor to numpy image for visualization\n    rgb_img = inv_normalize(image).permute(1, 2, 0).cpu().numpy()\n    rgb_img = np.clip(rgb_img, 0, 1)\n    \n    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n    \n    # Plot\n    plt.figure(figsize=(6, 4))\n    plt.imshow(visualization)\n    plt.title(f\"Predicted: {class_names[pred_label]} | Actual: {class_names[label]}\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion  \n\nThis project successfully implemented the **Swin Transformer (swin_tiny_patch4_window7_224)** architecture for **dental X-ray disease classification**.  \nOur approach followed a clear and systematic pipeline:  \n\n- **Data preparation**: Applied targeted image transformations to adapt grayscale dental X-rays for the model, ensuring optimal feature extraction.  \n- **Model configuration**: Fine-tuned a Swin Transformer backbone with a custom classification head matching the dataset’s number of classes.  \n- **Training strategy**: Used **Focal Loss** to address class imbalance and the **AdamW optimizer** for stable convergence.  \n- **Performance monitoring**: Validated after each epoch to track metrics and prevent overfitting.  \n- **Model preservation**: Saved the final trained weights in `.pth` format for seamless integration into the application.  \n\n**Key insights:**  \n- Swin Transformers are highly effective for medical imaging tasks, particularly when trained with carefully designed preprocessing and augmentation steps.  \n- Addressing **class imbalance** is critical in medical datasets, as rare conditions can be underrepresented without targeted loss functions or resampling.  \n- The trained model is now ready for **real-world integration**, enabling automated dental disease detection from X-ray images in a clinical or consumer application.  \n\n**Next steps:**  \n1. Integrate the `.pth` model into the app backend or mobile inference pipeline.  \n2. Optionally convert the model to **TorchScript** or **ONNX** for faster and more portable inference.  \n3. Explore additional improvements through **data augmentation**, **transfer learning**, and **hyperparameter optimization**.  \n","metadata":{}}]}